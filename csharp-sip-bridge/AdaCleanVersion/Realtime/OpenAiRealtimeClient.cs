using System.Buffers;
using System.Net.WebSockets;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using AdaCleanVersion.Session;
using Microsoft.Extensions.Logging;
using SIPSorcery.Net;

namespace AdaCleanVersion.Realtime;

/// <summary>
/// Bridges OpenAI Realtime API (WebSocket) â†” SIPSorcery RTPSession.
///
/// Audio flow:
///   Caller â†’ RTP (G.711 Âµ-law) â†’ decode to PCM16 â†’ base64 â†’ OpenAI input_audio_buffer.append
///   OpenAI response.audio.delta â†’ base64 â†’ PCM16 â†’ encode to G.711 Âµ-law â†’ RTP â†’ Caller
///
/// Transcript flow:
///   OpenAI conversation.item.input_audio_transcription.completed â†’ session.ProcessCallerResponseAsync
///   OpenAI response.audio_transcript.done â†’ logged only (AI output)
///
/// Session instructions:
///   CleanCallSession.OnAiInstruction â†’ session.update with new instructions
///
/// Key design:
///   - One instance per call, disposed on hangup
///   - No AI tools registered â€” voice-only
///   - Mic gate during AI speech to prevent echo
///   - Hard-cut barge-in via _clearEpoch guard on playout
/// </summary>
public sealed class OpenAiRealtimeClient : IAsyncDisposable
{
    private const string RealtimeUrl = "wss://api.openai.com/v1/realtime";
    private const int SendBufferSize = 4096;
    private const int ReceiveBufferSize = 16384;

    private readonly string _apiKey;
    private readonly string _model;
    private readonly string _voice;
    private readonly string _callId;
    private readonly RTPSession _rtpSession;
    private readonly CleanCallSession _session;
    private readonly ILogger _logger;
    private readonly CancellationTokenSource _cts = new();

    private ClientWebSocket? _ws;
    private Task? _receiveTask;

    // Mic gate: suppress caller audio while AI is speaking
    private volatile bool _micGated;

    // Playout epoch for barge-in hard-cut
    private volatile int _playoutEpoch;

    public event Action<string>? OnLog;

    public OpenAiRealtimeClient(
        string apiKey,
        string model,
        string voice,
        string callId,
        RTPSession rtpSession,
        CleanCallSession session,
        ILogger logger)
    {
        _apiKey = apiKey;
        _model = model;
        _voice = voice;
        _callId = callId;
        _rtpSession = rtpSession;
        _session = session;
        _logger = logger;
    }

    // â”€â”€â”€ Lifecycle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// <summary>
    /// Connect to OpenAI Realtime, configure session, and start bidirectional streaming.
    /// </summary>
    public async Task ConnectAsync()
    {
        _ws = new ClientWebSocket();
        _ws.Options.SetRequestHeader("Authorization", $"Bearer {_apiKey}");
        _ws.Options.SetRequestHeader("OpenAI-Beta", "realtime=v1");

        var url = $"{RealtimeUrl}?model={_model}";
        await _ws.ConnectAsync(new Uri(url), _cts.Token);

        Log("ðŸ”Œ Connected to OpenAI Realtime");

        // Configure session: voice, VAD, input transcription, no tools
        await SendSessionConfig();

        // Wire RTP â†’ OpenAI (caller audio in)
        _rtpSession.OnRtpPacketReceived += OnRtpPacketReceived;

        // Wire CleanCallSession instructions â†’ OpenAI session.update
        _session.OnAiInstruction += OnAiInstruction;

        // Start receive loop
        _receiveTask = Task.Run(ReceiveLoopAsync);

        Log("âœ… Bidirectional audio bridge active");
    }

    public async ValueTask DisposeAsync()
    {
        _cts.Cancel();

        _rtpSession.OnRtpPacketReceived -= OnRtpPacketReceived;
        _session.OnAiInstruction -= OnAiInstruction;

        if (_receiveTask != null)
        {
            try { await _receiveTask; } catch { /* swallow */ }
        }

        if (_ws?.State == WebSocketState.Open)
        {
            try
            {
                await _ws.CloseAsync(
                    WebSocketCloseStatus.NormalClosure, "call ended",
                    CancellationToken.None);
            }
            catch { /* best effort */ }
        }

        _ws?.Dispose();
        _cts.Dispose();

        Log("ðŸ”Œ OpenAI Realtime disconnected");
    }

    // â”€â”€â”€ Session Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    private async Task SendSessionConfig()
    {
        var systemPrompt = _session.GetSystemPrompt();

        var config = new
        {
            type = "session.update",
            session = new
            {
                modalities = new[] { "text", "audio" },
                voice = _voice,
                instructions = systemPrompt,
                input_audio_format = "pcm16",
                output_audio_format = "pcm16",
                input_audio_transcription = new { model = "whisper-1" },
                turn_detection = new
                {
                    type = "server_vad",
                    threshold = 0.5,
                    prefix_padding_ms = 300,
                    silence_duration_ms = 500
                },
                tools = Array.Empty<object>() // No tools â€” voice-only
            }
        };

        await SendJsonAsync(config);
        Log("ðŸ“‹ Session configured: VAD + whisper transcription, no tools");
    }

    // â”€â”€â”€ RTP â†’ OpenAI (Caller Audio In) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    private void OnRtpPacketReceived(
        IPEndPoint remoteEndPoint,
        SDPMediaTypesEnum mediaType,
        RTPPacket rtpPacket)
    {
        if (mediaType != SDPMediaTypesEnum.audio) return;
        if (_micGated) return; // Suppress echo during AI speech

        try
        {
            // Decode G.711 Âµ-law â†’ PCM16
            var payload = rtpPacket.Payload;
            var pcm16 = new byte[payload.Length * 2];
            for (int i = 0; i < payload.Length; i++)
            {
                var sample = MuLawDecode(payload[i]);
                pcm16[i * 2] = (byte)(sample & 0xFF);
                pcm16[i * 2 + 1] = (byte)((sample >> 8) & 0xFF);
            }

            // Send as base64 to OpenAI
            var b64 = Convert.ToBase64String(pcm16);
            var msg = new { type = "input_audio_buffer.append", audio = b64 };

            // Fire-and-forget (non-blocking for RTP thread)
            _ = SendJsonAsync(msg);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to forward RTP to OpenAI");
        }
    }

    // â”€â”€â”€ OpenAI â†’ RTP (AI Audio Out) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    private async Task ReceiveLoopAsync()
    {
        var buffer = new byte[ReceiveBufferSize];
        var msgBuffer = new MemoryStream();

        try
        {
            while (!_cts.IsCancellationRequested && _ws?.State == WebSocketState.Open)
            {
                var result = await _ws.ReceiveAsync(buffer, _cts.Token);

                if (result.MessageType == WebSocketMessageType.Close)
                {
                    Log("ðŸ”Œ WebSocket closed by server");
                    break;
                }

                msgBuffer.Write(buffer, 0, result.Count);

                if (!result.EndOfMessage) continue;

                var json = Encoding.UTF8.GetString(
                    msgBuffer.GetBuffer(), 0, (int)msgBuffer.Length);
                msgBuffer.SetLength(0);

                await HandleServerEvent(json);
            }
        }
        catch (OperationCanceledException) { /* expected on dispose */ }
        catch (Exception ex)
        {
            Log($"âš  Receive loop error: {ex.Message}");
        }
    }

    private async Task HandleServerEvent(string json)
    {
        using var doc = JsonDocument.Parse(json);
        var type = doc.RootElement.GetProperty("type").GetString();

        switch (type)
        {
            // â”€â”€ AI audio chunk â†’ decode and send via RTP â”€â”€
            case "response.audio.delta":
                HandleAudioDelta(doc.RootElement);
                break;

            // â”€â”€ AI started speaking â†’ gate the mic â”€â”€
            case "response.audio.started":
            case "response.created":
                _micGated = true;
                break;

            // â”€â”€ AI finished speaking â†’ ungate the mic â”€â”€
            case "response.audio.done":
                // Small delay to let playout drain before ungating
                await Task.Delay(200);
                _micGated = false;
                break;

            // â”€â”€ Caller transcript (from Whisper) â†’ feed to session â”€â”€
            case "conversation.item.input_audio_transcription.completed":
                await HandleCallerTranscript(doc.RootElement);
                break;

            // â”€â”€ AI transcript (what AI said) â†’ log only â”€â”€
            case "response.audio_transcript.done":
                var aiText = doc.RootElement.GetProperty("transcript").GetString();
                Log($"ðŸ¤– AI: {aiText}");
                break;

            // â”€â”€ VAD: speech started â†’ barge-in: hard-cut playout â”€â”€
            case "input_audio_buffer.speech_started":
                _micGated = false;
                Interlocked.Increment(ref _playoutEpoch); // Invalidate queued audio
                Log("ðŸŽ¤ Barge-in detected");
                break;

            // â”€â”€ Errors â”€â”€
            case "error":
                var errMsg = doc.RootElement.GetProperty("error")
                    .GetProperty("message").GetString();
                Log($"âš  OpenAI error: {errMsg}");
                break;

            case "session.created":
                Log("ðŸ“¡ Session created by server");
                break;

            case "session.updated":
                Log("ðŸ“‹ Session config accepted");
                break;
        }
    }

    private void HandleAudioDelta(JsonElement root)
    {
        var b64 = root.GetProperty("delta").GetString();
        if (string.IsNullOrEmpty(b64)) return;

        var epoch = _playoutEpoch; // Capture epoch before processing

        var pcm16 = Convert.FromBase64String(b64);

        // Encode PCM16 â†’ G.711 Âµ-law for RTP
        var mulaw = new byte[pcm16.Length / 2];
        for (int i = 0; i < mulaw.Length; i++)
        {
            var sample = (short)(pcm16[i * 2] | (pcm16[i * 2 + 1] << 8));
            mulaw[i] = MuLawEncode(sample);
        }

        // Hard-cut guard: if epoch changed (barge-in), drop this frame
        if (epoch != _playoutEpoch) return;

        // Send via RTP
        _rtpSession.SendAudioFrame(
            (uint)(mulaw.Length), // timestamp increment (8kHz, 1 sample = 1 unit)
            (int)SDPWellKnownMediaFormatsEnum.PCMU,
            mulaw);
    }

    private async Task HandleCallerTranscript(JsonElement root)
    {
        var transcript = root.GetProperty("transcript").GetString();
        if (string.IsNullOrWhiteSpace(transcript)) return;

        Log($"ðŸ‘¤ Caller: {transcript}");

        try
        {
            await _session.ProcessCallerResponseAsync(transcript, _cts.Token);
        }
        catch (Exception ex)
        {
            Log($"âš  Error processing transcript: {ex.Message}");
        }
    }

    // â”€â”€â”€ Instruction Updates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    private void OnAiInstruction(string instruction)
    {
        Log($"ðŸ“‹ Sending instruction update");

        var msg = new
        {
            type = "session.update",
            session = new
            {
                instructions = instruction
            }
        };

        _ = SendJsonAsync(msg);

        // Also create a response to make AI speak the instruction
        var responseMsg = new
        {
            type = "response.create",
            response = new
            {
                modalities = new[] { "text", "audio" }
            }
        };

        _ = SendJsonAsync(responseMsg);
    }

    // â”€â”€â”€ WebSocket Send â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    private readonly SemaphoreSlim _sendLock = new(1, 1);

    private async Task SendJsonAsync(object payload)
    {
        if (_ws?.State != WebSocketState.Open) return;

        var json = JsonSerializer.Serialize(payload);
        var bytes = Encoding.UTF8.GetBytes(json);

        await _sendLock.WaitAsync(_cts.Token);
        try
        {
            await _ws.SendAsync(
                bytes, WebSocketMessageType.Text, true, _cts.Token);
        }
        finally
        {
            _sendLock.Release();
        }
    }

    // â”€â”€â”€ G.711 Âµ-law Codec â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    private static readonly short[] MuLawDecompressTable = BuildMuLawDecompressTable();

    private static short MuLawDecode(byte mulaw) => MuLawDecompressTable[mulaw];

    private static byte MuLawEncode(short sample)
    {
        const int BIAS = 0x84;
        const int MAX = 32635;

        var sign = (sample >> 8) & 0x80;
        if (sign != 0) sample = (short)-sample;
        if (sample > MAX) sample = MAX;

        sample = (short)(sample + BIAS);

        var exponent = 7;
        for (var expMask = 0x4000; (sample & expMask) == 0 && exponent > 0; exponent--, expMask >>= 1) { }

        var mantissa = (sample >> (exponent + 3)) & 0x0F;
        var mulaw = (byte)(~(sign | (exponent << 4) | mantissa));

        return mulaw;
    }

    private static short[] BuildMuLawDecompressTable()
    {
        var table = new short[256];
        for (int i = 0; i < 256; i++)
        {
            var mulaw = (byte)~i;
            var sign = (mulaw & 0x80) != 0;
            var exponent = (mulaw >> 4) & 0x07;
            var mantissa = mulaw & 0x0F;
            var sample = (mantissa << 3) + 0x84;
            sample <<= exponent;
            sample -= 0x84;
            table[i] = (short)(sign ? -sample : sample);
        }
        return table;
    }

    private void Log(string msg)
    {
        _logger.LogInformation(msg);
        OnLog?.Invoke($"[RT:{_callId}] {msg}");
    }
}
